{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mind the backend\n",
    "This seminar was meant for taito-GPU cluster and __not the Everware__ nodes you used throughout the course.\n",
    "\n",
    "- One can still run it on Everware node after changing data paths, but it will probably take longer without a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random GPU roll:  3\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "gpuid = random.randint(0,3)\n",
    "import os\n",
    "print \"random GPU roll: \",gpuid\n",
    "os.environ[\"THEANO_FLAGS\"]=\"device=gpu%i\"%gpuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the challenge data\n",
    "* Currently we are using raw data features with no preprocessing in hope that NN figures that out\n",
    "* One may try any feature engineering he wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df = pd.DataFrame.from_csv(\"/homeappl/home/austyuzh/data/train.csv\",)\n",
    "\n",
    "test = pd.read_csv(\"/homeappl/home/austyuzh/data/public_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dphi(phi1, phi2):\n",
    "    \"Return the angular difference in the range [-pi,pi)\"\n",
    "    dphi = (phi1 - phi2 + np.pi) % (2 * np.pi) - np.pi\n",
    "    return dphi\n",
    "\n",
    "def dr(eta1, eta2, phi1, phi2):\n",
    "    \"Return the eta-phi distance\"\n",
    "    return np.sqrt((eta1-eta2)**2 + dphi(phi1,phi2)**2)\n",
    "\n",
    "real_objects = ['lepton', 'jet1', 'jet2', 'jet3', 'jet4']\n",
    "objects = real_objects + ['mem']\n",
    "\n",
    "def add_features(data):\n",
    "    data['ht'] = data['jet1_pt'] + data['jet2_pt'] + data['jet3_pt'] + data['jet4_pt']\n",
    "    data['meff'] = data['jet1_pt'] + data['jet2_pt'] + data['jet3_pt'] + data['jet4_pt'] + data['lepton_pt'] + data['mem_pt']\n",
    "    data['rt'] = data['meff'] / data['m_wwbb']\n",
    "    data['delta_m'] = np.abs(data['m_jjj'] - data['m_jlv'])\n",
    "    \n",
    "    variables = ['ht', 'meff', 'rt', 'delta_m']\n",
    "    \n",
    "    data['dr_j12'] = dr(data['jet1_eta'], data['jet2_eta'], data['jet1_phi'], data['jet2_phi'])\n",
    "    data['dr_j34'] = dr(data['jet3_eta'], data['jet4_eta'], data['jet3_phi'], data['jet4_phi'])\n",
    "    \n",
    "    variables += ['dr_j12', 'dr_j34']\n",
    "    \n",
    "    for i in [1,2,3,4]:\n",
    "        data['dr_lj%d' % i] = dr(data['jet%d_eta' % i], data['lepton_eta'], data['jet%d_phi' % i], data['lepton_phi'])\n",
    "        data['dphi_met_j%d' % i] = np.abs(dphi(data['mem_phi'], data['jet%d_phi' % i]))\n",
    "\n",
    "        variables += ['dr_lj%d' % i, 'dphi_met_j%d' % i]\n",
    "        \n",
    "    data['max_dr_lj'] = np.maximum(np.maximum(data['dr_lj1'], data['dr_lj2']),\n",
    "                                   np.maximum(data['dr_lj3'], data['dr_lj4']))\n",
    "    data['min_dr_lj'] = np.minimum(np.minimum(data['dr_lj1'], data['dr_lj2']),\n",
    "                                   np.minimum(data['dr_lj3'], data['dr_lj4']))   \n",
    "\n",
    "    data['max_dphi_met_j'] = np.maximum(np.maximum(data['dphi_met_j1'], data['dphi_met_j2']),\n",
    "                                        np.maximum(data['dphi_met_j3'], data['dphi_met_j4']))\n",
    "    data['min_dphi_met_j'] = np.minimum(np.minimum(data['dphi_met_j1'], data['dphi_met_j2']),\n",
    "                                        np.minimum(data['dphi_met_j3'], data['dphi_met_j4']))\n",
    "\n",
    "    variables += ['max_dr_lj', 'min_dr_lj', 'max_dphi_met_j', 'min_dphi_met_j']\n",
    "\n",
    "    data['max_jet_pt'] = np.maximum(np.maximum(data['jet1_pt'], data['jet2_pt']),\n",
    "                                    np.maximum(data['jet3_pt'], data['jet4_pt']))\n",
    "    data['min_jet_pt'] = np.minimum(np.minimum(data['jet1_pt'], data['jet2_pt']),\n",
    "                                    np.minimum(data['jet3_pt'], data['jet4_pt']))       \n",
    "    \n",
    "    variables += ['max_jet_pt', 'min_jet_pt']\n",
    "    \n",
    "    for obj in real_objects:\n",
    "        data['%s_abs_eta' % obj] = np.abs(data['%s_eta' % obj])\n",
    "        variables.append('%s_abs_eta' % obj)\n",
    "        \n",
    "        for obj2 in ['lepton']:\n",
    "            if obj == obj2:\n",
    "                continue\n",
    "            \n",
    "            dphi_name = \"dphi_%s_%s\" % (obj, obj2)\n",
    "            deta_name = \"deta_%s_%s\" % (obj, obj2)\n",
    "            \n",
    "            data[dphi_name] = np.abs(dphi(data['%s_phi' % obj], data['%s_phi' % obj2]))\n",
    "            data[deta_name] = np.abs(data['%s_eta' % obj] - data['%s_eta' % obj2])\n",
    "            \n",
    "            variables += [dphi_name, deta_name]\n",
    "    \n",
    "    return variables\n",
    "\n",
    "new_features = add_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ht',\n",
       " 'meff',\n",
       " 'rt',\n",
       " 'delta_m',\n",
       " 'dr_j12',\n",
       " 'dr_j34',\n",
       " 'dr_lj1',\n",
       " 'dphi_met_j1',\n",
       " 'dr_lj2',\n",
       " 'dphi_met_j2',\n",
       " 'dr_lj3',\n",
       " 'dphi_met_j3',\n",
       " 'dr_lj4',\n",
       " 'dphi_met_j4',\n",
       " 'max_dr_lj',\n",
       " 'min_dr_lj',\n",
       " 'max_dphi_met_j',\n",
       " 'min_dphi_met_j',\n",
       " 'max_jet_pt',\n",
       " 'min_jet_pt',\n",
       " 'lepton_abs_eta',\n",
       " 'jet1_abs_eta',\n",
       " 'dphi_jet1_lepton',\n",
       " 'deta_jet1_lepton',\n",
       " 'jet2_abs_eta',\n",
       " 'dphi_jet2_lepton',\n",
       " 'deta_jet2_lepton',\n",
       " 'jet3_abs_eta',\n",
       " 'dphi_jet3_lepton',\n",
       " 'deta_jet3_lepton',\n",
       " 'jet4_abs_eta',\n",
       " 'dphi_jet4_lepton',\n",
       " 'deta_jet4_lepton']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [u'lepton_pt', u'lepton_eta', u'lepton_phi', u'mem_pt', u'mem_phi',\n",
    "       u'jet1_pt', u'jet1_eta', u'jet1_phi', u'jet1_btag', u'jet2_pt',\n",
    "       u'jet2_eta', u'jet2_phi', u'jet2_btag', u'jet3_pt', u'jet3_eta',\n",
    "       u'jet3_phi', u'jet3_btag', u'jet4_pt', u'jet4_eta', u'jet4_phi',\n",
    "       u'jet4_btag', u'm_jj', u'm_jjj', u'm_lv', u'm_jlv', u'm_bb', u'm_wbb',\n",
    "       u'm_wwbb']\n",
    "\n",
    "all_features = features + new_features\n",
    "n_features = len(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y = df['target'].values==1\n",
    "#X = df[df.columns[1:]].values.astype(theano.config.floatX)\n",
    "\n",
    "test_X = test[df.columns[1:]].values.astype(theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,y,random_state=11, train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'lepton_pt',\n",
       " u'lepton_eta',\n",
       " u'lepton_phi',\n",
       " u'mem_pt',\n",
       " u'mem_phi',\n",
       " u'jet1_pt',\n",
       " u'jet1_eta',\n",
       " u'jet1_phi',\n",
       " u'jet1_btag',\n",
       " u'jet2_pt',\n",
       " u'jet2_eta',\n",
       " u'jet2_phi',\n",
       " u'jet2_btag',\n",
       " u'jet3_pt',\n",
       " u'jet3_eta',\n",
       " u'jet3_phi',\n",
       " u'jet3_btag',\n",
       " u'jet4_pt',\n",
       " u'jet4_eta',\n",
       " u'jet4_phi',\n",
       " u'jet4_btag',\n",
       " u'm_jj',\n",
       " u'm_jjj',\n",
       " u'm_lv',\n",
       " u'm_jlv',\n",
       " u'm_bb',\n",
       " u'm_wbb',\n",
       " u'm_wwbb',\n",
       " 'ht',\n",
       " 'meff',\n",
       " 'rt',\n",
       " 'delta_m',\n",
       " 'dr_j12',\n",
       " 'dr_j34',\n",
       " 'dr_lj1',\n",
       " 'dphi_met_j1',\n",
       " 'dr_lj2',\n",
       " 'dphi_met_j2',\n",
       " 'dr_lj3',\n",
       " 'dphi_met_j3',\n",
       " 'dr_lj4',\n",
       " 'dphi_met_j4',\n",
       " 'max_dr_lj',\n",
       " 'min_dr_lj',\n",
       " 'max_dphi_met_j',\n",
       " 'min_dphi_met_j',\n",
       " 'max_jet_pt',\n",
       " 'min_jet_pt',\n",
       " 'lepton_abs_eta',\n",
       " 'jet1_abs_eta',\n",
       " 'dphi_jet1_lepton',\n",
       " 'deta_jet1_lepton',\n",
       " 'jet2_abs_eta',\n",
       " 'dphi_jet2_lepton',\n",
       " 'deta_jet2_lepton',\n",
       " 'jet3_abs_eta',\n",
       " 'dphi_jet3_lepton',\n",
       " 'deta_jet3_lepton',\n",
       " 'jet4_abs_eta',\n",
       " 'dphi_jet4_lepton',\n",
       " 'deta_jet4_lepton']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[1:]].columns\n",
    "all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NN architecture\n",
    " * We take a simple NN with __3 hidden layers__:\n",
    "     * Layers must contain 500 hidden units each\n",
    "     * Layers must use __tanh__ nonlinearity\n",
    " * structure can be farther optimized, so feel free to experiment __after__ you got it working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_X = T.matrix('input X')\n",
    "target_Y = T.ivector('target Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer,DenseLayer,batch_norm,dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maxout(incoming, num_units, ds, **kwargs):\n",
    "    l1a = lasagne.layers.DenseLayer(incoming, nonlinearity=None, num_units=num_units * ds, **kwargs)\n",
    "    l1 = lasagne.layers.FeaturePoolLayer(l1a, ds=ds)\n",
    "    return l1\n",
    "\n",
    "l_in = InputLayer([None, n_features],input_X,'input layer')\n",
    "\n",
    "n_hid = 1000\n",
    "p_drop = 0.2\n",
    "\n",
    "l_0 = lasagne.layers.BatchNormLayer(l_in, name=\"l_0\")\n",
    "\n",
    "l_1 = batch_norm(DenseLayer(l_0,\n",
    "                num_units=n_hid,\n",
    "                name='dense0',\n",
    "                nonlinearity=lasagne.nonlinearities.tanh))\n",
    "\n",
    "d_1 = dropout(l_1, p=p_drop)\n",
    "\n",
    "l_2 = batch_norm(DenseLayer(d_1,\n",
    "                num_units=n_hid,\n",
    "                name='dense1',\n",
    "                nonlinearity=lasagne.nonlinearities.tanh))\n",
    "\n",
    "d_2 = dropout(l_2, p=p_drop)\n",
    "\n",
    "l_3 = batch_norm(DenseLayer(d_2,\n",
    "                num_units=n_hid,\n",
    "                name='dense2',\n",
    "                nonlinearity=lasagne.nonlinearities.tanh))\n",
    "\n",
    "d_3 = dropout(l_3, p=p_drop)\n",
    "\n",
    "l_4 = batch_norm(DenseLayer(d_3,\n",
    "                num_units=n_hid,\n",
    "                name='dense3',\n",
    "                nonlinearity=lasagne.nonlinearities.tanh))\n",
    "\n",
    "d_4 = dropout(l_4, p=p_drop)\n",
    "\n",
    "l_5 = batch_norm(DenseLayer(d_4,\n",
    "                num_units=n_hid,\n",
    "                name='dense4',\n",
    "                nonlinearity=lasagne.nonlinearities.tanh))\n",
    "\n",
    "nn = DenseLayer(l_5,num_units=2,\n",
    "                name='dense_out',\n",
    "                nonlinearity=lasagne.nonlinearities.softmax,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[l_0.beta,\n",
       " l_0.gamma,\n",
       " dense0.W,\n",
       " dense0_bn.beta,\n",
       " dense0_bn.gamma,\n",
       " dense1.W,\n",
       " dense1_bn.beta,\n",
       " dense1_bn.gamma,\n",
       " dense2.W,\n",
       " dense2_bn.beta,\n",
       " dense2_bn.gamma,\n",
       " dense3.W,\n",
       " dense3_bn.beta,\n",
       " dense3_bn.gamma,\n",
       " dense4.W,\n",
       " dense4_bn.beta,\n",
       " dense4_bn.gamma,\n",
       " dense_out.W,\n",
       " dense_out.b]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = lasagne.layers.get_all_params(nn,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_out = lasagne.layers.get_output(nn)\n",
    "loss = lasagne.objectives.categorical_crossentropy(nn_out, target_Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates =lasagne.updates.adadelta(loss,weights)\n",
    "train_fun = theano.function([input_X,target_Y],[loss,nn_out[:,1]],updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deterministic predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "det_nn_out = lasagne.layers.get_output(nn,deterministic=True)\n",
    "det_loss = lasagne.objectives.categorical_crossentropy(det_nn_out,target_Y).mean()\n",
    "val_fun = theano.function([input_X,target_Y],[det_loss,nn_out[:,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop\n",
    "* Almost identical to the original loop from previous seminar\n",
    "* The only difference is that now we keep track of NN performance across iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_auc_curve = []\n",
    "train_acc_curve = []\n",
    "val_auc_curve = []\n",
    "val_acc_curve = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50 took 506.058s\n",
      "  training loss:\t\t0.526843\n",
      "  training accuracy:\t\t73.26 %\n",
      "  training AUCscore:\t\t81.10 %\n",
      "  validation loss:\t\t0.508119\n",
      "  validation accuracy:\t\t73.88 %\n",
      "  validation AUCscore:\t\t81.87 %\n",
      "Epoch 2 of 50 took 509.999s\n",
      "  training loss:\t\t0.512087\n",
      "  training accuracy:\t\t74.26 %\n",
      "  training AUCscore:\t\t82.31 %\n",
      "  validation loss:\t\t0.496198\n",
      "  validation accuracy:\t\t74.59 %\n",
      "  validation AUCscore:\t\t82.71 %\n",
      "Epoch 3 of 50 took 509.603s\n",
      "  training loss:\t\t0.504950\n",
      "  training accuracy:\t\t74.74 %\n",
      "  training AUCscore:\t\t82.88 %\n",
      "  validation loss:\t\t0.490412\n",
      "  validation accuracy:\t\t74.93 %\n",
      "  validation AUCscore:\t\t83.10 %\n",
      "Epoch 4 of 50 took 512.379s\n",
      "  training loss:\t\t0.499936\n",
      "  training accuracy:\t\t75.08 %\n",
      "  training AUCscore:\t\t83.27 %\n",
      "  validation loss:\t\t0.485585\n",
      "  validation accuracy:\t\t75.28 %\n",
      "  validation AUCscore:\t\t83.47 %\n",
      "Epoch 5 of 50 took 506.706s\n",
      "  training loss:\t\t0.496176\n",
      "  training accuracy:\t\t75.34 %\n",
      "  training AUCscore:\t\t83.56 %\n",
      "  validation loss:\t\t0.481820\n",
      "  validation accuracy:\t\t75.41 %\n",
      "  validation AUCscore:\t\t83.69 %\n",
      "Epoch 6 of 50 took 502.930s\n",
      "  training loss:\t\t0.493125\n",
      "  training accuracy:\t\t75.54 %\n",
      "  training AUCscore:\t\t83.79 %\n",
      "  validation loss:\t\t0.479489\n",
      "  validation accuracy:\t\t75.62 %\n",
      "  validation AUCscore:\t\t83.87 %\n",
      "Epoch 7 of 50 took 493.353s\n",
      "  training loss:\t\t0.490526\n",
      "  training accuracy:\t\t75.71 %\n",
      "  training AUCscore:\t\t83.99 %\n",
      "  validation loss:\t\t0.477546\n",
      "  validation accuracy:\t\t75.74 %\n",
      "  validation AUCscore:\t\t84.00 %\n",
      "Epoch 8 of 50 took 490.680s\n",
      "  training loss:\t\t0.488338\n",
      "  training accuracy:\t\t75.85 %\n",
      "  training AUCscore:\t\t84.15 %\n",
      "  validation loss:\t\t0.475020\n",
      "  validation accuracy:\t\t75.92 %\n",
      "  validation AUCscore:\t\t84.21 %\n",
      "Epoch 9 of 50 took 486.384s\n",
      "  training loss:\t\t0.486446\n",
      "  training accuracy:\t\t75.99 %\n",
      "  training AUCscore:\t\t84.29 %\n",
      "  validation loss:\t\t0.473396\n",
      "  validation accuracy:\t\t75.95 %\n",
      "  validation AUCscore:\t\t84.29 %\n",
      "Epoch 10 of 50 took 483.476s\n",
      "  training loss:\t\t0.484633\n",
      "  training accuracy:\t\t76.12 %\n",
      "  training AUCscore:\t\t84.42 %\n",
      "  validation loss:\t\t0.471482\n",
      "  validation accuracy:\t\t76.11 %\n",
      "  validation AUCscore:\t\t84.41 %\n",
      "Epoch 11 of 50 took 485.962s\n",
      "  training loss:\t\t0.483082\n",
      "  training accuracy:\t\t76.23 %\n",
      "  training AUCscore:\t\t84.54 %\n",
      "  validation loss:\t\t0.469836\n",
      "  validation accuracy:\t\t76.25 %\n",
      "  validation AUCscore:\t\t84.55 %\n",
      "Epoch 12 of 50 took 591.060s\n",
      "  training loss:\t\t0.481715\n",
      "  training accuracy:\t\t76.31 %\n",
      "  training AUCscore:\t\t84.64 %\n",
      "  validation loss:\t\t0.468581\n",
      "  validation accuracy:\t\t76.29 %\n",
      "  validation AUCscore:\t\t84.61 %\n",
      "Epoch 13 of 50 took 731.585s\n",
      "  training loss:\t\t0.480408\n",
      "  training accuracy:\t\t76.39 %\n",
      "  training AUCscore:\t\t84.73 %\n",
      "  validation loss:\t\t0.467291\n",
      "  validation accuracy:\t\t76.40 %\n",
      "  validation AUCscore:\t\t84.70 %\n",
      "Epoch 14 of 50 took 820.805s\n",
      "  training loss:\t\t0.479337\n",
      "  training accuracy:\t\t76.47 %\n",
      "  training AUCscore:\t\t84.81 %\n",
      "  validation loss:\t\t0.465979\n",
      "  validation accuracy:\t\t76.46 %\n",
      "  validation AUCscore:\t\t84.78 %\n",
      "Epoch 15 of 50 took 758.301s\n",
      "  training loss:\t\t0.478213\n",
      "  training accuracy:\t\t76.54 %\n",
      "  training AUCscore:\t\t84.89 %\n",
      "  validation loss:\t\t0.465667\n",
      "  validation accuracy:\t\t76.46 %\n",
      "  validation AUCscore:\t\t84.84 %\n",
      "Epoch 16 of 50 took 598.658s\n",
      "  training loss:\t\t0.477029\n",
      "  training accuracy:\t\t76.62 %\n",
      "  training AUCscore:\t\t84.98 %\n",
      "  validation loss:\t\t0.464004\n",
      "  validation accuracy:\t\t76.51 %\n",
      "  validation AUCscore:\t\t84.91 %\n",
      "Epoch 17 of 50 took 667.324s\n",
      "  training loss:\t\t0.476155\n",
      "  training accuracy:\t\t76.68 %\n",
      "  training AUCscore:\t\t85.04 %\n",
      "  validation loss:\t\t0.462584\n",
      "  validation accuracy:\t\t76.62 %\n",
      "  validation AUCscore:\t\t84.97 %\n",
      "Epoch 18 of 50 took 606.167s\n",
      "  training loss:\t\t0.475223\n",
      "  training accuracy:\t\t76.74 %\n",
      "  training AUCscore:\t\t85.10 %\n",
      "  validation loss:\t\t0.462434\n",
      "  validation accuracy:\t\t76.62 %\n",
      "  validation AUCscore:\t\t84.97 %\n",
      "Epoch 19 of 50 took 664.619s\n",
      "  training loss:\t\t0.474352\n",
      "  training accuracy:\t\t76.80 %\n",
      "  training AUCscore:\t\t85.17 %\n",
      "  validation loss:\t\t0.462950\n",
      "  validation accuracy:\t\t76.60 %\n",
      "  validation AUCscore:\t\t85.07 %\n",
      "Epoch 20 of 50 took 673.315s\n",
      "  training loss:\t\t0.473542\n",
      "  training accuracy:\t\t76.85 %\n",
      "  training AUCscore:\t\t85.22 %\n",
      "  validation loss:\t\t0.460391\n",
      "  validation accuracy:\t\t76.76 %\n",
      "  validation AUCscore:\t\t85.11 %\n",
      "Epoch 21 of 50 took 672.819s\n",
      "  training loss:\t\t0.472608\n",
      "  training accuracy:\t\t76.91 %\n",
      "  training AUCscore:\t\t85.29 %\n",
      "  validation loss:\t\t0.459365\n",
      "  validation accuracy:\t\t76.79 %\n",
      "  validation AUCscore:\t\t85.16 %\n",
      "Epoch 22 of 50 took 666.405s\n",
      "  training loss:\t\t0.471991\n",
      "  training accuracy:\t\t76.94 %\n",
      "  training AUCscore:\t\t85.33 %\n",
      "  validation loss:\t\t0.458980\n",
      "  validation accuracy:\t\t76.87 %\n",
      "  validation AUCscore:\t\t85.23 %\n",
      "Epoch 23 of 50 took 498.852s\n",
      "  training loss:\t\t0.471274\n",
      "  training accuracy:\t\t77.00 %\n",
      "  training AUCscore:\t\t85.38 %\n",
      "  validation loss:\t\t0.458042\n",
      "  validation accuracy:\t\t76.91 %\n",
      "  validation AUCscore:\t\t85.27 %\n",
      "Epoch 24 of 50 took 498.131s\n",
      "  training loss:\t\t0.470565\n",
      "  training accuracy:\t\t77.05 %\n",
      "  training AUCscore:\t\t85.43 %\n",
      "  validation loss:\t\t0.457787\n",
      "  validation accuracy:\t\t76.89 %\n",
      "  validation AUCscore:\t\t85.28 %\n",
      "Epoch 25 of 50 took 498.300s\n",
      "  training loss:\t\t0.469848\n",
      "  training accuracy:\t\t77.08 %\n",
      "  training AUCscore:\t\t85.48 %\n",
      "  validation loss:\t\t0.457856\n",
      "  validation accuracy:\t\t76.83 %\n",
      "  validation AUCscore:\t\t85.32 %\n",
      "Epoch 26 of 50 took 497.724s\n",
      "  training loss:\t\t0.469224\n",
      "  training accuracy:\t\t77.13 %\n",
      "  training AUCscore:\t\t85.53 %\n",
      "  validation loss:\t\t0.456079\n",
      "  validation accuracy:\t\t76.95 %\n",
      "  validation AUCscore:\t\t85.35 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-766727b0e22d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mYpred_batches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/homeappl/home/austyuzh/miniconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 1000\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_err = 0\n",
    "    Ypred_batches = []\n",
    "    Ytrue_batches = []\n",
    "    train_batches = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        err, y_pred = train_fun(inputs, targets)\n",
    "        \n",
    "        Ypred_batches.append(y_pred)\n",
    "        Ytrue_batches.append(targets)\n",
    "        \n",
    "        train_err += err\n",
    "        train_batches += 1\n",
    "    \n",
    "    Ypred_train = np.concatenate(Ypred_batches)\n",
    "    Ytrue_train = np.concatenate(Ytrue_batches)\n",
    "    train_acc = accuracy_score(Ytrue_train, Ypred_train>0.5)\n",
    "    train_auc = roc_auc_score(Ytrue_train, Ypred_train)\n",
    "    \n",
    "    train_acc_curve.append(train_acc)\n",
    "    train_auc_curve.append(train_auc)\n",
    "\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    Ypred_batches = []\n",
    "    Ytrue_batches = []\n",
    "    val_batches = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, y_pred = val_fun(inputs, targets)\n",
    "        Ypred_batches.append(y_pred)\n",
    "        Ytrue_batches.append(targets)\n",
    "        \n",
    "        val_err += err\n",
    "        val_batches += 1\n",
    "\n",
    "    Ypred_val = np.concatenate(Ypred_batches)\n",
    "    Ytrue_val = np.concatenate(Ytrue_batches)\n",
    "    val_acc = accuracy_score(Ytrue_val, Ypred_val>0.5)\n",
    "    val_auc = roc_auc_score(Ytrue_val, Ypred_val)\n",
    "    \n",
    "    val_acc_curve.append(val_acc)\n",
    "    val_auc_curve.append(val_auc)\n",
    "\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc * 100))\n",
    "    print(\"  training AUCscore:\\t\\t{:.2f} %\".format(\n",
    "        train_auc * 100))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc  * 100))\n",
    "    print(\"  validation AUCscore:\\t\\t{:.2f} %\".format(\n",
    "        val_auc * 100))\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAG3CAYAAACg3AOFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XuYXVVh///P2ud+ztwvuU0uIOGOBFBTFPs1EovQarEi\nfhPv/mgB/aFV20cr3mItorR9fn79WYtUxAsIxfaXfm0rCF9pvFJBDCAhgXAJSSaT65y5nfvee/3+\n2OecOTM5s2cSEmaSvF/Ps5619t5r7bNnMs/Dh3XW3ttYawUAAACgOWe2LwAAAACYywjMAAAAQAgC\nMwAAABCCwAwAAACEIDADAAAAIQjMAAAAQIgZBWZjzKXGmC3GmKeNMZ9ocrzDGPP/GWMeM8b8tzHm\nrJmOBQAAAOYyM91zmI0xjqSnJa2WtEvSw5LWWGu3NPS5SdKotfYLxpjTJf2DtfYNMxkLAAAAzGUz\nmWFeKWmrtfYFa21F0l2SLp/U5yxJD0iStfYpSScZY3pnOBYAAACYs2YSmPsk7WjY3lnd1+gxSW+V\nJGPMSklLJS2e4VgAAABgzjpSN/19SVKnMea3kv5vSRsleUfo3AAAAMCsic6gT7+CGeOaxdV9ddba\nUUn/V23bGPO8pOckpacb2zAmfDE1AAAAcARYa82h9J9JYH5Y0nJjzDJJA5LWSFrb2MEY0y4pb62t\nGGP+TNJPrbVjxphpx066+EO5dhzn1q1bp3Xr1s32ZWCO4e8CzfB3gWb4u0AzxhxSVpY0g8BsrfWM\nMddJuk/BEo5brbWbjTHXBIftLZLOlPQdY4wvaZOkq8LGHvJVAgAAALNkJjPMstbeK+n0Sfu+0dD+\n78nHw8YCAAAAxwre9Ic5a9WqVbN9CZiD+LtAM/xdoBn+LnCkTPvikpeKMcbOlWsBAADA8ckYc8g3\n/THDDAAAAIQgMAMAAAAhCMwAAABACAIzAAAAEILADAAAAIQgMAMAAAAhCMwAAABACAIzAAAAEILA\nDAAAAIQgMAMAAAAhCMwAAABACAIzAAAAEILADAAAAIQgMAMAAAAhCMwAAABACAIzAAAAEILADAAA\nAIQgMAMAAAAhCMwAAABACAIzAAAAEILADAAAAIQgMAMAAAAhCMwAAABACAIzAAAAEILADAAAAIQg\nMAMAAAAhCMwAAABACAIzAAAAEILADAAAAIQgMAMAAAAhCMwAAABACAIzAAAAEILADAAAAIQgMAMA\nAAAhCMwAAABACAIzAAAAEILADAAAAIQgMAMAAAAhCMwAAABACAIzAAAAEILADAAAAIQgMAMAAAAh\nCMwAAABACAIzAAAAEILADAAAAIQgMAMAAAAhCMwAAABACAIzAAAAEILADAAAAIQgMAMAAAAhCMwA\nAABAiOhsXwAAAABwNFlr5VqrsrWHNZ7ADAAAgEPiW6uy76tSDaFl359QVyZt1/fPoO/kPpUjdCxq\njGLGHNbPS2AGAACYI2w13JWqQa/k+03bk0Pn5Lrk+9P2aaxLM+jXGGw9SXFjFHecCXWsyb6444Tv\nN0Yxx1GiWqccR/FYrH6sfu6Qz5ruWMwYOdWwfDiReUaB2RhzqaSvKFjzfKu19suTjrdJul3SUkkR\nSX9vrf129dg2ScOSfEkVa+3Kw7hOAACAQ1b7Kr5SKw2zorV2rZQbtxva5Un9Kg2zlo3nq4XU0jTt\n6cJw1BgljFHCcRSvBslaO15tJ5qEz6nqpOOo7RD6JxpCZ2KKsOtIsrJyfbdeKn5lwvbBpTT1scp4\n2/O9CccKvqsx6x00ZnI/13flNes3ad/hMHaatRzGGEfS05JWS9ol6WFJa6y1Wxr6fFJSm7X2k8aY\nHklPSZpvrXWNMc9JeoW1NjvN59jprgUAABxb/CazpcVqKTS0p9pX8Lxp+zQdV/2sirWKSPVZxlr4\nq7UP2m4Ii83GRI1krC/HepL1JOtK1pWxniLWkyNfEXnV464c68nYioz1ZPxK0N+vBMWWZb2KrF+W\n9UuyfkW+X5LrVerhs+JXVKluV7xKPQD61pe1Vr71JxSrJvtm0K9ZH9/6TQNorTjGUdSJNi0xJzbl\nsWYl4kQUc2KKOJHx/Wb82EH9TZN9M+gXcSK6/IzLZa09pInmmcwwr5S01Vr7giQZY+6SdLmkLQ19\nrKTWartV0gFrbS3CG/E0DgAAXjJuk0BZsvagfcVJAXZysC1NmhWdvD2Tvq61E2ZFE8YoFYkoWZ35\nTFa/gk9OKo372qNRLai2E46jmKyi8hWVJ0eeItaVY10ZvyLHupJflrGufK+siltQyS2o4BZUqExR\nN9k3PEXfildRMppUKpZSKpqq14loQjEnplgkVg+LtXYsEquHyGZ94rV9sRZFnY7m4xrGRJ2ojDFy\njDOhGDXZN6nfTPo09gsLuI45ceLdTAJzn6QdDds7FYToRl+T9ENjzC5JLZL+Z8MxK+l+Y4wn6RZr\n7T+9iOsFAGDO86yd8cxo2P6pSmma/VZSqhouJwfRZMP+hDGKypcjrxpArSLVMBqplqh8tVhP7fJk\nrCenOmuqakiV78qxFUkVWb8iYyuytizZiqxfkueX5flufXa04ldU9soqeSWVvXLQdksa88o6MGlf\ns35lryzHOIpH4kpEE4pH4kE7kmi6b3KwbWx3JDsmHptBnYgkZA7zxjEcu47UTX9vlLTRWnuxMeYU\nBQH5XGvtmKSLrLUDxpje6v7N1tpfHKHPBQDgIN4Us6lhYfNwS7Ow61lbnyFtNnuadJyDZlkbZ1Zb\nIhF1x2ITxiaMCWZN/aJ8tyjPK8h1y6q4OVUqOZXKoypXxlQsjyhfGdVYYUyj5VGNlcc0Vh7TaClo\n727YV6gUlIln1BJvUSKSmDArWms3znA2ftU+oV+zfdX+qVjmoLG1UFsLuTMNv7V2xInM9p8YTjAz\nCcz9Cm7mq1lc3dfo/ZJulCRr7bPGmOclnSHpN9bager+fcaY9Qpmp5sG5nXr1tXbq1at0qpVq2b0\nQwAAjh2etcp5nvKep5zvH9TOeZ7yU7TDjhV9rxqIbfUO/lqxispWv8b3gplT68mRK8d3q+tLKzJ+\nsJZUXknWL8n3ivK9IJh6bkGuW5Dr5uX7xeCrf78iUz2HYytqsa7abFmO70nyGu7ID2rPGOUk5avb\nZtLxxu1au+SWJoTeeCSu1nirWuItaom3qDURtCfsq7bnZ+Y3P96wLxVLnVBfq+PEtGHDBm3YsOFF\nnWMmN/1FFNzEt1rSgKSHJK211m5u6PMPkvZaaz9vjJkv6TeSVkgqSnKstWPGmIyk+yR93lp7X5PP\n4aY/AJhFtnqnf74aRPOep0K1zjepG4/lPE9jbkVjXkVjrqu87ynnuvVjBd+qYK2KvuRKSsgqbnzF\n5ClqPUVtRRFbkWMrcvySjF+SvJLkFeR7BfluXr6bk+uOyXVzqpRHVa6MqFwekVsZU8JIqUhthjai\nRCSqVDSp5FEoUSda/31Z2Xpb0hHdtrJKRpP1oNsSb6l/NoDDZ4w58jf9WWs9Y8x1CsJu7bFym40x\n1wSH7S2S/kbSt40xj1eHfdxaO2iMOVnSemOMrX7WHc3CMgBgao1BNizA5lxX2UpBI5Wihisljbhl\njboVjbkV5T233rdopZKVStaobI0qclRRRK4cGVlFbTBj6lhXjl+WsWUZryT5RVmvVgryvbw8tyDP\nzclz83JsWTHrK2Z8xeUrbqSksco4jnoco0wkokwkonQ0pnR1LWnt5qn6TVS1djStVKy72k5NuMlq\ncjseibOmFMBRNe0M80uFGWYAxzJbXTOb8zyNNZSc79fbI25Fw5WSsuWihtyShitljbquRj1XuWqg\nLfpWRd/Ww2w5WDggI1t9EkBZxi9L1RlYW52B9dy8fK+gqHUVk6+4sUoYKWmklGOUijjKOBGlI1Fl\nqutn046jTDSqjBNVSzSqlkhM6VhCiUhCieih13y1D+BYcDgzzARmACccv7qGdjzIuhpygyCbLZeC\nMOuWNVwJyqjnatR1g7Wyvq+8b1XwpWI10JblqKKojHxF/HIQar2i5BVkvbysW5DnjsmrjCmqimLy\nlJCvpLFKGqt07UavaEQtkahaI3G1xuJqi8XVHk2qI55SWzyjTDyjdCytTCxo1+ravmQ0yUwrAEyD\nwAzguFJ0i8oWstqTP6D+fFa78sPaWxzTiFvRiOfWZ29znlXBt8pbqeBLJTkqWUclRVQxUVUUlWti\n8py4PBOXdeLBLK1XkLy8rJuX/KIifkkRW1bUrygmVzG546HWGGUijtIRR62RqFojUbXH4uqIJdQe\nS6gzllJbPF0PsulY+qCAm4wmmYUFgFlGYAYwp5R9X4OVknaMDWpXYUj9hWHtLYxqXzmv/aWCspWS\nhtyKRjxPOd8q7xuV5KismFwnLhtJS9EWyUQU9UuK2YoSchWXp6TxlTBWSVmlHSntGKWdINS2RiJq\niUTUGo2pPRpTezSujlhcHfGkOqMJdcZTSseCFw0ko0klIgkeUwUAJwgCM4Ajxlqrgu/rQKWs/vyw\n+gsj6i+MaE8xp33lgvZXSspWXA27rkZ9KedbFayjkqKqmJhcJyHJkbycHK+oqC0pIVdJ+Uo7Vi2O\no7ZoRO3RmLpjCXUnUuqNZ7Qg1aaFqTb1pTu1INmi9mhUScdhqQEA4Ig4Kk/JAHDs8nxf+0o57chn\ntSM/rN2FUe0u5bS3VNCBSkmDlYqGPU8jnlXOGhVsREXFVHEScp2UJF+qjMrx8or6RSVsWUm5yhhf\nLRGp3XG0KBZVVzSu3kRa85IZLUy2qS/docWZTs1Pdakt2cYyBADAMY0ZZmCWVLyK8pW88pW8Cm4h\nqCuFg/blywUNVwo6UKko67kacoOAO+pLY9Yo7zvKK6qSialkEqqYpNxISl4kHSxp8IrNA69j1RaJ\nqDMaUVcsrt5EUvPiaS1MtmhRuk1L0h2al+pUW6KN5QoAgOMGSzKAWWSt1WBhULtGd2lgbEC7RncF\n7dEB7Rrbpf7RAfXnR5R1PRUUlaKtiie7FUt0KRLvkBNrl6JtUqxVfiQtL5JWxUmpbBIyklKqKGU8\ntdTCrmPUHnHUEY2oKxpTdzymnnhS8+JJzUukNT+R1pJMp1piqdn+1QAAMGcQmIGjoBaEG0PwrpEB\nvTC2R9vzw+ovjmlPqahB11c02a1Mpk+JZK8i8S7ZWJvKTloFxZW3jlojjrpjMXXHYuqKxdUVjaoz\nGlVXLDaxjkbVWW13RqNKRZjhBQDgSCAwAzPkW1/DxWFli1kNFYe0O7dfz4zs0bNjB7SjMKJdxZz2\nlsvKup5GbUROvFPxZK9MvFN+tEVlk5IxRu2OVVc0ovmJpPqSGc2PJ9Ubj6snFlNPLKbeat0Ti6kr\nGlXUYS0vAACzicCME4rruxoqDmmoOKRsIatsMTuhHixmtbuQ0x7X1f6Kp6xvNGKjyimhUiSjSHKe\nnHi3/FiH/EhacVtSi3HV4Rh1x6JamEipL9WqkzIdWpTMHBSC08z6AgBwzCEw45hkrVWuktPe3F7t\nGdujvbm99TJYGAwCcXE8CB8ojSnrGRVMUqnMYqXSixRLLZCT6JEf65AbbVPRSStvEkrIqiNi1Rt1\ntCAe16JEUktTLTop3aZFiaQWxONaGI+rOxaTw2PLAAA47hGYMWd4vqf9+f314LsnNzEIN27vGdsj\nY4zmtfSpvW250q0nKZ7uk+K98mJtKjkZ5UxSIzaqId+obKV5sZj6qoF3clmYSGhBPK75sZiSzAID\nAIAGBGYcVSW3pIGxAe0Z23NQ4N2b3zthhjhbzKoz2al5mXma3zJfPZn5ymSWKppaJCXnqRztUMHJ\naEhx7XeN+ssVDbquFsbjWpJIaEkyqcWJhBZWZ4AbA3FHNMpLLAAAwGEhMOOwWGt1oHBA/SP92jW6\nS/2j/eof6Q/qhvZIaUQLWhZoQcuCIAhn5mteZp560vOUSC+QjfeqFG1XzqQ1aGPaVS5rR6mkHcWi\n9lYq6o3F6mF4SSIxXqrheEE8rghBGAAAHEUEZhyk6BaDEBwShneN7lI6llZfW5/6WoOyqHVRfXte\n6yIpPk+DJqXniiW9UCxqZ6lUD8MD5bI6otEpw/CS6kxxjCdEAACAWUZgPgHty+3T43se13PZ55qG\n4ZHSiBa2LgyCcEMgrrfbgnAciST1fKGgZ5qUHaWSFiUSWp5K6ZRkUstqobha9yUSShCGAQDAMYDA\nfByreBU9feBpPbbnMT22+zE9vvdxPbb7MeUreZ07/1wt71p+UBDua+1Tb6ZXjgnCbN7z9NwUoXh3\nuaylyaROSaW0fFI5KZkkEAMAgOMCgfk4sT+/X4/vCQLxY3se0+N7HteW/Vu0uG2xVixYoXPnnasV\nC1ZoxfwVWtq+dMINcCOuq2enCMWDrquTk8mDAvHyVEpLEwleqgEAAI57BOZjjOu7emr/U0E4rgbj\nx/Y8prHymM6df65WzF9Rr8+Zd44y8Ux97Jjr6rFcThtHR7VxbExb8nk9UyhozPOaBuJTUin1JRLc\nVAcAAE5oBOY57ED+QD0Q18Lx5n2b1dfWpxXzV4yH4wUrtKx92YRZ433lsjaOjQWlGpB3lko6O5PR\n+S0tOr+lRWdlMjo1ldKCeJxHrgEAAEyBwDwH+NbXM4PPaOPARj26+9H6WuOR0sjEWeMFwaxxS7yl\nPtZaqxeKxfFwXA3IOd/XedVgXCtnpNMsoQAAADhEBOaXWNEt6om9T+jR3Y8GAXnPo3p8z+PqSffo\n/AXnBzPH1bXGyzqW1W++kyTX9/VUoVCfMd44NqZHx8aUcpwgFLe21sPxSckks8YAAABHAIH5KMoW\nsnpsz2PaOLBRG3cHs8dbB7fq1K5Tdf7C83Xe/PN0/sIgJHemOieMLXiefpfLTVhS8UQup75EYnzW\nuBqQ58Xjs/QTAgAAHP8IzEeAtVY7R3YGs8a7x8Px/vx+nTv/XJ2/4Hydt+A8nb/gfJ0972wlo8mD\nzrGtUNB92ax+PjysjaOjeq5Y1Onp9IQlFStaWtQajc7CTwgAAHDiIjAfIs/39NSBp+pLKmrhOOJE\ndP6C88fD8cLzdUrnKYo4kabnGXNdbRga0o+zWd03OKgh19UlXV1a1dGhC1padHYmozjrjQEAAGYd\ngXkae3N79W9b/k2/HfitNu7eqCf2PqFFrYvqM8a1emHrwtDz+Nbq0bEx/XhwUPdls/rN6KhWtrbq\nkq4uvbGzU+e2tMhhzTEAAMCcQ2Buwlqrn73wM938yM2695l79Yen/qEu7LtQ5y04TysWrFBbom1G\n5xkolXR/NqsfDw7q/mxWXdGo3tjVpTd2del1HR3KRJrPPgMAAGDuIDA3yBay+u5j39XNj9wsxzi6\n9hXX6t0r3q2OZMeMxhc9T78YHq4vs9hRKml1Z6cu6ezUJV1dWpY8eO0yAAAA5rYTPjBba/VQ/0O6\n+ZGbtX7zev3RaX+ka19xrV679LXTPpbNWqvN+Xx9mcUvh4f18kymvszila2tPPcYAADgGHfCBubR\n0qju+N0d+sYj39BoaVTXvOIave+896k30xs67kClop9Ul1ncl80qItWXWVzc0aGOWOywrgcAAABz\n0wkXmB/d/ahu/s3N+udN/6yLT75Y177iWq1+2eoJLwhpZK3Vr0ZGdO/goO4bHNSWfF7/o6NDl3R2\n6o1dXTo1leIFIQAAAMexEyIw5yt53b3pbt38m5u1a3SX/uyCP9NVF1ylRa2LQsc9PjamD2/dqt3l\nsv6kt1dv7OzUq9vblWCZBQAAwAnjuA7Mm/dt1jce+YZuf/x2Xbj4Ql37ymt12fLLpnw2cs1gpaLP\nbdumu/fu1bqTTtLVixYpwiwyAADACelwAvOcftVcyS1p/Zb1uvk3N+upA0/pqvOv0iNXP6JlHcum\nHetZq1sHBvSZ55/XFb29enLlSnWzJhkAAACHaE4G5ueyz+mWR27RbY/eppfPe7muW3mdLj/9csUi\nMwu8vxoe1oe2blU6EtGPzz1X57W2HuUrBgAAwPFqTgXm9ZvX6+ZHbtZvB36r9654r37+/p/rtO7T\nZjx+oFTSJ557Tg9ks7rplFO0dt48buIDAADAizKn1jBfdOtFuvaV1+ptZ71NyejMXwxS9n39r507\n9eXt2/WnCxfq08uWqSU6p/5fAAAAAHPAcX3T31R+PDioP9+6VaekUvrK8uU6NZ0+ClcHAACA48Fx\nd9NfmOcKBX30mWe0KZfTV5Yv15t6emb7kgAAAHAcOuYCc97zdOP27frH/n79xZIluvvss3mWMgAA\nAI6aYyYwW2v1g3379JfPPqvXtrfr0Ve+UouTM1/nDAAAAByOYyIwPzE2pg8/84wOVCq6/cwz9T86\nOmb7kgAAAHCCmNOBOVt9S99dtbf0LVyoKMsvAAAA8BKak+nTs1bf3LVLZz70kMq+rydf9Sp9sK+P\nsAwAAICX3JybYf7v4WFdt3WrEo6jH517ri7gLX0AAACYRXMqML9v82bdn83qyy97md45fz5v6QMA\nAMCsm1OBeV48ri0rV6qVt/QBAABgjjjm3/QHAAAAzNThvOmPu+gAAACAEARmAAAAIASBGQAAAAhB\nYAYAAABCEJgBAACAEARmAAAAIASBGQAAAAhBYAYAAABCzCgwG2MuNcZsMcY8bYz5RJPjbcaYHxpj\nHjXG/M4Y876ZjgUAAADmsmnf9GeMcSQ9LWm1pF2SHpa0xlq7paHPJyW1WWs/aYzpkfSUpPmS/OnG\nNpyDN/0BAADgqDpab/pbKWmrtfYFa21F0l2SLp/Ux0pqrbZbJR2w1rozHAsAAADMWTMJzH2SdjRs\n76zua/Q1SWcZY3ZJekzSnx/CWAAAAGDOOlI3/b1R0kZr7SJJ50v6B2NMyxE6NwAAADBrojPo0y9p\nacP24uq+Ru+XdKMkWWufNcY8L+mMGY6tW7duXb29atUqrVq1agaXBwAAADS3YcMGbdiw4UWdYyY3\n/UUU3MS3WtKApIckrbXWbm7o8w+S9lprP2+MmS/pN5JWSBqebmzDObjpDwAAAEfV4dz0N+0Ms7XW\nM8ZcJ+k+BUs4brXWbjbGXBMctrdI+htJ3zbGPF4d9nFr7WD1og4aeygXCAAAAMymaWeYXyrMMAMA\nAOBoO1qPlQMAAABOWARmAAAAIASBGQAAAAhBYAYAAABCEJgBAACAEARmAAAAIASBGQAAAAhBYAYA\nAABCEJgBAACAEARmAAAAIASBGQAAAAhBYAYAAABCEJgBAACAEARmAAAAIASBGQAAAAhBYAYAAABC\nEJgBAACAEARmAAAAIASBGQAAAAhBYAYAAABCEJgBAACAEARmAAAAIASBGQAAAAhBYAYAAABCEJgB\nAACAEARmAAAAIASBGQAAAAhBYAYAAABCEJgBAACAEARmAAAAIASBGQAAAAhBYAYAAABCEJgBAACA\nENHZvgAAAAAc56yVPE9yXalSCerGdqUSlHL56JfDQGAGAACY61xXKpXGy+RwebS2G8PtVGF3umOV\nShCWIxEpGg1KLDberpVEQorHD7+k09P3icWk//qvQ/71E5gBAMCJy1rJ9yfOdIbNgk51rFyWisUg\nzE5Xz6TP5NpaKZkMQmVjsIzFmrdneqylZepjsdh4sG0MuJPD7lTHGtuRiOS8tCuBXVcaHZWGh6WR\nkfH6cBCYAQDA0VcLpYcSLF9sXSrNLPA6zsSA1xD0bCwmG4nKj8TkO1H5TkyeE5XnxOQpKldRVUxM\nnompEklWS0IVJ6mKk1DZSapsUiqZTpVMQmWTVEkJFZVU0VbrSEKFdFKFZEKFtqTyXkIFG9Q5L6iL\nbrR+2VJwycYEN6M5ruT4klOWnELDMWfqEna88VgyObGkUoe3r7YdjwfnD2OtlM+Ph9zJgbexDjtW\nKEitrVJbm9TePl4fDmOtPbyRR5gxxs6VawEA4Jgz+Sv7yTOZU5Vicfo1n2FrS6dbd1o77nnjs5a1\nmdJqbZNJ2XhCNpGUjSXkJ5LyY4lqScqLJeTFkvKjCbnRpLxq7UaCuhJJyI0EAbUSSapsEipHkirZ\nuAqVmPLlqPKVmHKlqMZK1boY1WgxprFiVPmio3w+CGmFgia0C4Xxb/vT6SD41dqN24nE+ITs5InZ\n6fYdSt9oNAicvh+U2gT5VCXs+HRjPW/8/2+KxeB3UWs3257Jvkpl/J++MVRHoxNng+Pxg4NurW62\nr9mxTKb5pLYxRtbaaWL7pDFzJaQSmAEAxyzXPTglhCWIQykzDb3S+Nf1k0vjV/nNjsXj4etHJ39l\n31BsLK68G1c2F9fgaEwHRuPaPxLXvuG49g7FtScb1+7BuPbsj2j/AaPBwSCENS59tfbg5awvptSW\nysZizcNts+2p2slkcD4cGb7f/M+8Uglmg9vbgzoeP3rXQGAGAJxYrA0CYz5/ZNaJTjHWFkvy8kX5\nhWDblIpySkWZctCWtbLJVDBDOqkokZRNVutEMJ1mk8HUWm27Vmy1NtWAa1JJmWRCTiohJ50M6lRC\nJtkk9EaPzCpLz5MOHJD275f27RuvG9uT60hE6u2VenqCurE9ue7qCsJQY8CtLQMAXgoEZgDA3OG6\n499tH80Sj49/t9s4m5oMvtZ3o9W1pJGkykrU148W/ITyfrA+dMxNaqyS0GglqZFSQsOlpIaLCWWL\nSQ3lExopJxXNJBRtCc5dMkkVTSpYf6qkXEVlbZDfJdXbYWUm/Rq/Gve88W1jgpBau4+qWXu67cnt\noaEg/A4PS52dBwfdqUJwT08wEwscKwjMAIDmfH98cWZtiUBtgWa12PzEWsWibKEgUyjIFgpSviAV\nJ45RsShTGN/f2Ja1UjojP5WWnwyKl6iWeFpuPK1KLCjlaFBK0bRKkbRKTlrFaskrrYIJ6rzSytmg\njPlpjXkpFSsRFYvB+seRkYnF2vE1jS+mpNNzZwa0FqRrAboxTNfa0203O9bREYTgzk6WIOD4RmAG\ngGNApSKNjliN7C8rt7+g/P68ioN5eWMFeWMF+WN52XxBfi4IqqYecgtyigU5xbwi5YKcckGRckGx\ncl7RSkFRt6C4m1fcKyjuFZTwCkr6eSVsQTFVgllVpWZUSg19i2a8rrVLJnnQvlq7VlxFFU+Y+nLX\n6R6xOpMAZn7BAAAgAElEQVRHsE7VJ5FoHowTidn+1wYw1xCYAeDFqq2JzeUm3DJvc0GozR8oqDiY\nVymbV2U4r/JwQe5oXv5oXn4uL5srSIW8nEJeTikItrFKXvFKXnEvr4RfUG2u1FNERSetciQVzLBG\nUnKjKVViabmxlNx4Sl48JS+Rlp9IySZSwTrZVHBXkklXSyYtJ5NSpKVaWtOKtqYUa6uW9rQSbQkl\nkkaJxPjsYW3G1Ji5M3sKAEcbgRnAcadQkAYHpWw2qIeHrNyxomwuLzuWqwdbkw9qp5CTU8zJKeQV\nKeaCUsorWsopWs4pVs4rVs4p5uYVr+SUqOSCIOvmlPRySnh5eSaqgpNR0aSUU0Zjflo5P6WSk1Y5\nlpYbCwKsl0zLJoNb6Z1MSk5LWk5rWrG2tGJtKSU600p0ppXsSivdnVKmN61Mb1qp7rRMOnXEbtIC\nAMwcgRnAnOS5VkMDBQ3vHNVo/4hyu0eV3z2i0r4RlQ+MysuOyBselRkZkRkbVTQ/onhpRInSqFo1\noo7IqNo0olY7opQ3Js+JqRTNqBTNqBJNqxzPqBLLqBJPqxLPBOtjExm5yYz8RFp+KhOsoU1lZNMZ\nKR20TSYtZTIyLUHbac3IaUkrlooqFgu+zq99td/SQr4FgOMBgRnAkee69dcp+YNDGts5pLH+YeV3\nDam4Z1iVfUPyBodlR0bkjI0okhtVrBiE3VRlRGl/VK12RBXFlHdalY+1qRhvk5tslZtuk9/SJrW2\nyuloU7SzVbGeNiV725Sa16qWRW1K9LaNv6qplly5IwkAcJgIzAAmqr1fdHg4eGbU0FC97WeHVdwd\nhN7y3iG5g8NSdkhmZFjRsSHFCsNKlYYU9woaddo1rHYN+h3KRdtVSHSokmqX29Ih29Yu09GuSFe7\nYl2tSvS2KbWgTZn5rWrta1Pb4ja19bXKScRm+7cBAACBGZjrrA2e5lV7u5XnNa8n7/OKFSmblckO\nyhkalDOclTM8qOjwoCIjg4qOZBUbHVRsdFDxsUHFclklcoOK54fkOTHl4x1B6DUdyvrt2l/p0L5K\nuwrxDrmZdvltHUHo7e5QrKddyQUdSi9sV+uSDnUsblF3j1FPT/DCAZYlAACOZQRmYBa4bvC2qz17\nxsvevU3au60K+8bUE8mqxxlUt5NVtxlUtxlUlwbVabLqsoPqsIPq8AfV7mfV4Q2qzRtUwi9oNNqp\n0ViXRmNdGot3KRfvVC7epVyiS/lkUArJThXSXSqmulRMdSrW26GOefH6ywV6eqTubhF+AQAnLAIz\ncIQUixNDb2PwHdxVVGFXVqXdWfn7BxUdzWpJZlBLWrJamMxqfiwIwx12UK1uVulSMNsbHc1K8bhM\nZ2eQVhvLdPtaW3nuFwAARwCBGWjg+8GbvyYt3a3Xuf0FuQP7pL17Zfftl78/eHaZM5JVW2VwUvjN\nqtUdVLqUleO7cls7ZTu7FOnuVLS3U6a7IeB2dk5sN+7jLQoAAMwqAjOOK6XS1GG3WZ0fLMo5sE+x\noX1KjuxVS2GfFsf3qi++TwujezVP+9Rj96rT3af24l5FbEWFll6V2ubJ7eyR09Ol+LxOJRd1KbGg\nU6ZritCbyTDbCwDAMYrAjGNCPi8NDARl166p6/JYWae07dPJmb1akgzC74LIPs3TXnX7+9RZ2avW\n0j5l8vuUGt2rSKUor7NXfu88OfN7FVnQK2f+PKm3V5o3qe7tDR5RRvAFAOCEcjiBmVt+cMTk8+EB\nuFZHC6M6t3dAZ7QP6NSWAS2N79ZKZ0C97oA6CwPKlAYUNwMyZlQm0SN1Ngu8Jx+8r71dDgEYAAAc\nYQRmTKkSPMmsXoaGgnr37oND8K5+q0xpsB6ET8kM6GWxAV2kAfW4A+rIDyidG1DcHZAivkxiodS5\nUFrYWM6cuN3dLTnObP8aAADACY4lGccxa6VCYTzoNgu/U21ns1K5LHV0SPM6yjottUOnxbfpZOcF\nLYn0a6EG1FMeUHt+QKnhAcUO7JZSKZmFk0Nwk8JSCAAAMEuO2hpmY8ylkr4iyZF0q7X2y5OO/6Wk\nd0qykmKSzpTUY60dMsZskzQsyZdUsdaunOIzCMyHIJ+XNm+WnnhC2rRJ6u9vHn6NGb9XrVY6OiZu\nd2VKWuRu1/zCNnWNvqDWA9uU3rtN0f5tMtu2Sfv2BUH3pJOkZcukvr7mQTiVmu1fCwAAQKijEpiN\nMY6kpyWtlrRL0sOS1lhrt0zR/02SPmKtfUN1+zlJr7DWZqf5HAJzE6WStGVLEIo3bZoYkE87TTrn\nHOnss6WlS5uH4WRSwUOFt2+Xtm2bWF54Iaj37w9C8EknjZdly8bbfX284QIAABwXjtZNfyslbbXW\nvlD9kLskXS6paWCWtFbSnY3XpWBmGiEqFWnr1omh+Ikngkx78snjwfg97wnq5csbMmyxOB6Cf7dt\nPAjXSjYrLV48MRBfdtl4e9EiKRJ56X9oAACAY8BMZpivkPRGa+3V1e13SVpprf1wk74pSTslnWKt\nHarue07SkCRP0i3W2n+a4nNOiBlmz5OeffbgGeNnnpGWLAnCcC0cn3NOMIscj1cHl8vS00+PD6qV\n7duDwVPNEC9YQCAGAADQ3His3Jsl/aIWlqsustYOGGN6Jd1vjNlsrf3FEf7cOcf3g4neyTPGTz0l\nzZ8/Hoj/6I+kT3xCOuOMhiXArjs+3fyvDcn6+eeDIFwbvGZN0D711IZUDQAAgCNpJoG5X9LShu3F\n1X3NrNHE5Riy1g5U633GmPUKlng0Dczr1q2rt1etWqVVq1bN4PJml+cFqx6efDLItU8+GZQtW4L1\nxLXZ4osvlj70Iemss6SWlobBzz0XDPzPhlnjrVuDdcO1wW95i/SpT0mnn15dlAwAAICZ2LBhgzZs\n2PCizjGTJRkRSU8puOlvQNJDktZaazdP6tcu6TlJi621heq+tCTHWjtmjMlIuk/S56219zX5nDm9\nJMN1x7NtLRQ/+WQwY9zbGwThs88O6rPOks48U2pvrw6uTTdPXkqxZUvw0o1aMK7NHJ9xhpROz+rP\nCwAAcDw62o+V+18af6zcl4wx10iy1tpbqn3eq2Ct8zsaxp0sab2Cx81FJd1hrf3SFJ8xJwJzpRKs\nJ54cjLduDZ6cNjkYn3GG1NracALXlX73O+mXv5QeeSQIyZs3T5xurgXjM8+cNBgAAABH01ELzC+F\nlzowl0rB/XONofjJJ4NZ5CVLxgNxrZx+upTJNDnR8LD03/8t/epXQUh+6KFgOcVFF0mvepX08pcH\nJ+joeMl+NgAAADRHYA4xNCStXy/9x38Es8fbtgWPa5scjE87LeT9G9YGN97VwvEvfxkk7Fe8QnrN\na4KQ/OpXB690BgAAwJxDYJ5kbEz693+X7rpL2rBBesMbpD/5E+m88yY9rm0q5bK0ceN4OP7Vr4JX\n51100XhAPu88nlABAABwjCAwK3iHx733BiH53nuDXLt2rXT55VJb2zSDDxwIQnFtBvm3vw3eEFIL\nxxddFDzWzRzS7xgAAABzxAkbmCsV6Sc/CULyD38YTPquWSO99a1ST88Ug6wNHnHRuLxi1y7p935v\nPBz/3u/NIGUDAADgWHFCBWbfl37+8yAk/+u/SqecEoTkK68M3vTc1O7d0ne/K/3iF0FQbmkZD8ev\neU1wgx5vxAMAADhuHfeB2Vrp4YeDkHz33cHzj9eskd7+9uAGvint2CH97d9Kt98eJOo3vCEIyH19\nR/aHAAAAwJw2F16NfcRZGzzK+M47g6AciwVrku+/P3iMcajnn5e+9CXpBz+QrroqeG7cggUvyXUD\nAADg+DBnA/PWrUFAvuuu4GkXa9YESy/OO28G99w9/bR0443BIzKuvTbYnnIxMwAAADC1ORWYt28P\nllrceWdw/93b3y5985vShRfO8MEUmzZJN9wg/Z//I33oQ8Er+3hhCAAAAF6EObWGubvb6q1vDWaT\nX/e6Q7j/buNG6W/+JnjSxUc/Kn3wg7xyGgAAAAc55m/6K5Xsob0D5Ne/DoLyb38r/eVfSldfPcX7\nqwEAAIDj4Ka/GYfln/9c+sIXgucof+ITwU19yeRRvTYAAACcmOZUYA5lbfB2ki98Qdq5U7r+eund\n7+a11AAAADiq5n5gtlb60Y+CpRfZrPSpTwXPlYvO/UsHAADAsW/upk7fl/73/w6CcqUiffrT0hVX\n8CY+AAAAvKTmXmD2vGBN8g03BMstPvMZ6Y//WHKc2b4yAAAAnIDmVmD+7nelL35R6uqSbrpJuvTS\nGT6AGQAAADg65tRj5ezrXhfMKF98MUEZAAAAR9wx/xzmuXItAAAAOD4dTmBmYTAAAAAQgsAMAAAA\nhCAwAwAAACEIzAAAAEAIAjMAAAAQgsAMAAAAhCAwAwAAACEIzAAAAEAIAjMAAAAQgsAMAAAAhCAw\nAwAAACEIzAAAAEAIAjMAAAAQgsAMAAAAhCAwAwAAACEIzAAAAEAIAjMAAAAQgsAMAAAAhCAwAwAA\nACEIzAAAAEAIAjMAAAAQgsAMAAAAhCAwAwAAACEIzAAAAEAIAjMAAAAQgsAMAAAAhCAwAwAAACEI\nzAAAAEAIAjMAAAAQgsAMAAAAhCAwAwAAACEIzAAAAEAIAjMAAAAQgsAMAAAAhCAwAwAAACEIzAAA\nAEAIAjMAAAAQYkaB2RhzqTFmizHmaWPMJ5oc/0tjzEZjzG+NMb8zxrjGmI6ZjAUAAADmMmOtDe9g\njCPpaUmrJe2S9LCkNdbaLVP0f5Okj1hr33AoY40xdrprAQAAAF4MY4ysteZQxsxkhnmlpK3W2hes\ntRVJd0m6PKT/Wkl3HuZYAAAAYE6ZSWDuk7SjYXtndd9BjDEpSZdK+tdDHQsAAADMRUf6pr83S/qF\ntXboCJ8XAAAAmBXRGfTpl7S0YXtxdV8zazS+HONQx2rdunX19qpVq7Rq1aoZXB4AAADQ3IYNG7Rh\nw4YXdY6Z3PQXkfSUghv3BiQ9JGmttXbzpH7tkp6TtNhaWziUsdW+3PQHAACAo+pwbvqbdobZWusZ\nY66TdJ+CJRy3Wms3G2OuCQ7bW6pd3yLpx7WwHDb2UC4QAAAAmE3TzjC/VJhhBgAAwNF2tB4rBwAA\nAJywCMwAAABACAIzAAAAEILADAAAAIQgMAMAAAAhCMwAAABACAIzAAAAEILADAAAAIQgMAMAAAAh\nCMwAAABACAIzAAAAEILADAAAAIQgMAMAAAAhCMwAAABACAIzAAAAEILADAAAAIQgMAMAAAAhCMwA\nAABACAIzAAAAEILADAAAAIQgMAMAAAAhCMwAAABACAIzAAAAEILADAAAAIQgMAMAAAAhCMwAAABA\nCAIzAAAAEILADAAAAIQgMAMAAAAhCMwAAABACAIzAAAAEILADAAAAIQgMAMAAAAhCMwAAABACAIz\nAAAAEILADAAAAIQgMAMAAAAhCMwAAABACAIzAAAAEILADAAAAISIzvYFTOekk07SCy+8MNuXgRPU\nsmXLtG3bttm+DAAAMIuMtXa2r0GSZIyxza7FGKO5co048fD3BwDA8aX633ZzKGNYkgEAAACEIDAD\nAAAAIQjMAAAAQAgC8yz46U9/qiVLltS3zznnHP3sZz+bUd9D9YEPfEA33HDDYY8HAAA40c35p2Qc\nr4wZX2v+xBNPzLhvmO985zv65je/qZ///Of1ff/4j/94eBcIAAAAScwwH1estTMO18c6z/Nm+xIA\nAMAJgsB8mG666SZdeeWVE/b9+Z//uT7ykY9Ikr797W/rrLPOUltbm5YvX65bbrllynOdfPLJeuCB\nByRJxWJR73vf+9TV1aVzzjlHDz/88IS+X/7yl7V8+XK1tbXpnHPO0b/9279JkrZs2aIPfOADevDB\nB9Xa2qquri5J0vvf/3599rOfrY//p3/6J5166qnq6enRW97yFg0MDNSPOY6jb3zjGzrttNPU1dWl\n6667bsprfvjhh/Wa17xGnZ2d6uvr04c+9CG5rls/vmnTJl1yySXq7u7WwoUL9aUvfUmS5Pu+vvjF\nL9Z/hle96lXq7+/XCy+8IMdx5Pt+/Ryvf/3r9a1vfUtSMHv+2te+Vh/72MfU09Ojz3/+83ruuee0\nevVq9fT0aN68eXrXu96lkZGR+vidO3fqiiuu0Lx589Tb26sPf/jDqlQq6u7u1qZNm+r99u3bp0wm\nowMHDkz58wIAgBMXgfkwrVmzRvfcc49yuZykIAj+4Ac/0Dvf+U5J0vz58/WjH/1IIyMjuu222/TR\nj35Ujz766LTnXbdunZ5//nk9//zz+vGPf6zvfOc7E44vX75cv/zlLzUyMqLPfe5zete73qU9e/bo\njDPO0M0336xXv/rVGh0d1eDg4EHnfuCBB3T99dfrX/7lXzQwMKClS5dqzZo1E/r853/+px555BE9\n9thjuvvuu3Xfffc1vc5IJKKvfOUrGhwc1IMPPqgHHnhAX//61yVJY2Nj+oM/+AP94R/+oQYGBvTM\nM89o9erVkqS///u/1z//8z/r3nvv1cjIiL71rW8pnU5Lmn7pya9//WstX75ce/fu1ac+9SlZa3X9\n9ddr9+7d2rx5s3bu3Kl169ZJCv493vSmN+nkk0/W9u3b1d/frzVr1igWi2nt2rW6/fbb6+e98847\n9YY3vEHd3d2hnw8AAE5MBObDtHTpUl1wwQVav369JOknP/mJMpmMXvWqV0mSLrvsMp100kmSpN//\n/d/XJZdcMmFt8VR+8IMf6NOf/rTa29vV19enD3/4wxOOX3HFFZo/f74k6corr9Spp56qhx56aEbX\n/P3vf19XXXWVVqxYoVgsphtvvFEPPvigtm/fXu/zyU9+Uq2trVqyZIle//rXTxnyL7jgAq1cuVLG\nGC1dulRXX321fvrTn0qS/uM//kMLFy7URz7yEcXj8Qm/l1tvvVU33HCDli9fLkl6+ctfrs7Ozhld\nf19fnz74wQ/KcRwlEgmdcsopWr16taLRqLq7u/XRj360fg2//vWvNTAwoJtuuknJZFLxeFyvec1r\nJEnvec979P3vf79+3u9973t697vfPaNrAAAAJ55jPjAbc2TK4Vi7dq3uvPNOScEs5Tve8Y76sXvu\nuUevfvWr1d3drc7OTt1zzz3av3//tOfctWuXFi9eXN9etmzZhOPf/e53df7556uzs1OdnZ3atGnT\njM5bO3fj+TKZjLq7u9Xf31/fVwvjkpROpzU2Ntb0XFu3btWb3/xmLVy4UB0dHfrUpz5Vv44dO3bo\nlFNOaTpux44detnLXjaj651s8tNC9u7dq7Vr12rx4sXq6OjQu971rvo17Ny5U8uWLZPjHPwnvnLl\nSmUyGf30pz/VU089pWeffVZ//Md/fFjXBAAAjn/HfGC29siUw3HllVdqw4YN6u/v1/r16+uBuVwu\n621ve5s+/vGPa9++fcpms7rssstm9IrlhQsXaseOHfXtF154od7evn27rr76an39619XNptVNpvV\n2WefXT/vdEsaFi1aNOF8uVxOBw4cmBDQZ+oDH/iAzjzzTD377LMaGhrSDTfcUL+OJUuW6Nlnn206\nbunSpU2PZTIZSVI+n6/v271794Q+k3++66+/Xo7jaNOmTRoaGtLtt98+4Rq2b98+YU10o/e+9736\n3ve+p+9973t629vepng8PsOfHAAAnGiO+cA8m3p6evS6171O73//+/Wyl71Mp59+uqQgMJfLZfX0\n9MhxHN1zzz1TrgWe7O1vf7tuvPFGDQ0NaefOnfra175WP5bL5eQ4jnp6euT7vm677bYJj6SbP3++\ndu7cqUql0vTca9eu1W233abHH39cpVJJ119/vS688MLDes7z6Oio2tralE6ntWXLlgmPr3vTm96k\n3bt366tf/arK5bLGxsbqy0auuuoqfeYzn9EzzzwjSfrd736nbDarnp4e9fX16fbbb5fv+/rWt741\nZehuvIaWlha1traqv79ff/u3f1s/tnLlSi1cuFB/9Vd/pXw+r1KppF/96lf14+985zu1fv163XHH\nHXrPe95zyD8/AAA4cRCYX6R3vOMd+slPflK/2U+SWlpa9NWvflVXXnmlurq6dNddd+nyyy+f8hyN\nM6ef+9zntHTpUp188sm69NJLJ4S5M888U3/xF3+hCy+8UAsWLNCmTZv02te+tn784osv1tlnn60F\nCxZo3rx5B33O6tWr9YUvfEFvfetb1dfXp+eff1533XVX0+tott3o7/7u73THHXeora1N11xzzYSb\nB1taWnT//ffrhz/8oRYsWKDTTjtNGzZskCR97GMf09vf/nZdcsklam9v15/+6Z+qUChIkm655Rbd\ndNNN6unp0ebNm3XRRRdN+fm139Ujjzyijo4OvfnNb9YVV1xRP+Y4jv793/9dW7du1dKlS7VkyRLd\nfffd9eOLFy/WBRdcIGPMhN8hAADAZGYmywReCsYY2+xajDEzWsoAHKqrrrpKfX19+uu//usp+/D3\nBwDA8aX63/ZDuoNtRm/6M8ZcKukrCmakb7XWfrlJn1WS/h9JMUn7rLWvr+7fJmlYki+pYq1deSgX\nCBwN27Zt0/r167Vx48bZvhQAADDHTbskwxjjSPqapDdKOlvSWmPMGZP6tEv6B0lvstaeI6nxjR6+\npFXW2vMJy5gLPvvZz+rcc8/Vxz/+8YOeQgIAADDZtEsyjDEXSvqctfay6vZfSbKNs8zGmA9IWmit\n/WyT8c9LeqW1NvQ1aizJwFzE3x8AAMeXw1mSMZOb/vok7WjY3lnd1+g0SV3GmP8yxjxsjGl8C4SV\ndH91/58dysUBAAAAs21Ga5hneJ4LJF0sKSPpQWPMg9baZyRdZK0dMMb0KgjOm621vzhCnwsAAAAc\nVTMJzP2SljZsL67ua7RT0n5rbVFS0RjzM0krJD1jrR2QJGvtPmPMekkrJTUNzOvWrau3V61apVWr\nVs3spwAAAACa2LBhQ/3xtodrJmuYI5KekrRa0oCkhySttdZubuhzhqT/V9KlkhKSfi3pf0raJsmx\n1o4ZYzKS7pP0eWvtQW/xYA0z5iL+/gAAOL4clcfKWWs9Y8x1CsJu7bFym40x1wSH7S3W2i3GmB9L\nelySJ+kWa+2TxpiTJa03xtjqZ93RLCzj/2/v7qOrqM49jn+fIxENEHqSaJKGJAQiCk0tpsUXFMvL\nKiqVhUbhSnjNqtWFWhHU3ojyIl6r0pQW9dJWSSBcRBTEIlZECxWlC4RWefElKCgQIAFjAgkxAib7\n/pHjMYGcQwLBE8jvs1YWZ/bs2fPMsNfkOZM9s0VERESkudLEJSGwatUqhg8fTkFBzbOUqampzJw5\nk6uvvvq4dRtrzJgxdOjQgQcffPCkYm6pzsT+JyIi0pKdsolLpOnVnnb6gw8+aHDdYPLy8pg1axbv\nvPOOv+zPf/7ziQUoIiIiIkDDXisnpwnnXIOTaxERERFpGCXMJ2jatGkMHjy4TtnYsWO55557AJgz\nZw7dunUjIiKClJQUnnnmmYBtJScns3LlSgC+/vprRo8eTWRkJKmpqaxfv75O3SeeeIKUlBQiIiJI\nTU3lb3/7GwD5+fmMGTOGNWvW0K5dOyIjIwHIzMxk0qTv5pN59tlnueCCC4iOjuaGG26gsLDQv87j\n8fDXv/6VLl26EBkZyV133RUw5vXr19OzZ0+8Xi/x8fH85je/4ZtvvgFgx44deDweqqur/fX79OlD\nbm5unTi+PT+pqals2LAh4L5EREREQkkJ8wm65ZZbWLZsGRUVFQBUV1ezcOFChg0bBkBMTAyvvfYa\nZWVlzJ49m3HjxjUoKZwyZQqff/45n3/+OcuXLycvL6/O+pSUFP71r39RVlbG5MmTGT58OHv37uWi\niy7iL3/5C1dccQXl5eWUlJQc0/bKlSuZMGECixYtorCwkMTERG655ZY6df7+97/zn//8h40bN/Li\niy/yxhv1P6N51lln8ac//YmSkhLWrFnDypUrmTlzpn99sDvdCxcuZOrUqcybN4+ysjJeeeUVoqKi\njntuREREREJBCfMJSkxMJC0tjZdffhmAFStW0KZNG3r06AHAddddR8eOHQHo1asX/fv3rzO2OJCF\nCxfy0EMP0b59e+Lj47n77rvrrL/pppuIiYkBYPDgwVxwwQWsW7euQTHPnz+fX/3qV/zkJz8hLCyM\nxx57jDVr1rBz505/nQceeIB27dqRkJBAnz59Aib5aWlpXHrppZgZiYmJ3HbbbaxatapBceTk5PDb\n3/6WtLQ0ADp16kRCQkKDthURERH5vp32D/3Zw00zZtdNbvybEIYOHcrzzz/P8OHDef7558nIyPCv\nW7ZsGVOnTuWTTz6hurqayspKLr744uO2uWfPHjp06OBfTkpKqrN+7ty5/PGPf2T79u0AVFRUUFxc\n3KB49+zZw09/+lP/cps2bYiKimL37t0kJtbMTfNtMg4QHh7OwYMH623r008/Zfz48fz73/+msrKS\nb775pk7bwRQUFNC5c+cG1RUREREJtdM+YT6RRLepDB48mPvuu4/du3fz8ssvs3btWgAOHz7MzTff\nzLx58xg0aBAej4cbb7yxQa8ni4uLo6CggK5duwI144G/tXPnTm677Tb++c9/csUVVwBwySWX+Ns9\n3gN/P/zhD+u0V1FRwZdfflknQW+oMWPGkJaWxgsvvEB4eDgzZszgpZdeAmoScYCvvvqKtm3bAlBU\nVOTfNiEhgW3btjV6nyIiIiKhoCEZJyE6Opqf//znZGZm0qlTJy688EKgJmE+fPgw0dHReDweli1b\nFnAs8NGGDBnCY489xv79+9m1axdPP/20f11FRQUej4fo6Giqq6uZPXt2nVfSxcTEsGvXLo4cOVJv\n20OHDmX27Nls2rSJQ4cOMWHCBC6//PITGg5RXl5OREQE4eHh5Ofn13l9XXR0NPHx8cybN4/q6mpy\nc3PrJMi33nor2dnZvPfeewBs27atzrAQERERkeZECfNJysjIYMWKFf6H/QDatm3Lk08+yeDBg4mM\njFgTJucAABIHSURBVGTBggUMGjQoYBu17wxPnjyZxMREkpOTufbaaxk5cqR/XdeuXbn33nu5/PLL\niY2N5cMPP+Sqq67yr+/bty8/+tGPiI2N5fzzzz9mP/369eORRx4hPT2d+Ph4Pv/8cxYsWFBvHPUt\n15adnc1zzz1HREQEt99++zEPDz777LNMmzaN6OhoPv74Y6688kr/uptvvpkHH3yQjIwMIiIiuPHG\nGyktLQ24LxEREZFQ0kx/IkGo/4mIiJxZTmSmP91hFhEREREJQgmziIiIiEgQSphFRERERIJQwiwi\nIiIiEoQSZhERERGRIJQwi4iIiIgEoYRZRERERCQIJcwiIiIiIkEoYQ6xMWPG8OijjzZ5XRERERFp\nGprp7yQkJyeTk5ND3759Qx2KnCLNuf+JiIhI42mmv2amqqoq1CGcFnSeREREpDlTwnyCRo4cyc6d\nOxk4cCARERFkZ2ezY8cOPB4Pubm5JCUl0a9fPwCGDBlCXFwcXq+X3r1789FHH/nbyczMZNKkSQCs\nWrWKhIQEpk+fTkxMDPHx8cyZM+eE6paUlDBw4EDat2/PZZddxsSJE+nVq1fA4wkW49dff829995L\nx44d8Xq9XH311Rw6dAiA1atXc+WVV+L1eklKSmLu3LkA9OnTh9zcXH8beXl5dfbv8XiYOXMmXbp0\noUuXLgDcc889JCYm0r59e3r06MHq1av99aurq/nd735HSkoKERER9OjRg927d3PXXXdx33331TmW\nQYMGMWPGjCD/eyIiIiINp4T5BM2dO5fExEReffVVysrK6iRtb7/9Nvn5+SxfvhyAAQMGsG3bNvbt\n20daWhrDhg0L2G5RURHl5eXs2bOHWbNmceedd3LgwIFG173jjjto164d+/btY86cOeTl5WEW+K8P\nwWK89957ef/991m7di0lJSVMmzYNj8fDzp07GTBgAGPHjqW4uJgNGzbQvXv3gPs4ev9Llixh/fr1\n/uT80ksvZdOmTZSWlpKRkcHgwYM5fPgwAH/4wx944YUXeP311ykrKyM3N5fw8HBGjRrFggUL/G1+\n+eWXrFixIug5FhEREWkU51yz+KkJ5ViBypuDjh07uhUrVviXt2/f7jwej9u+fXvAbUpLS52ZubKy\nMuecc6NHj3YTJ050zjn31ltvufDwcFdVVeWvf/7557t33323UXWrqqpcWFiY+/TTT/3rHnroIder\nV68GHVftGKurq925557rNm/efEy9xx57zKWnp9fbRu/evV1OTo5/ec6cOXX2b2burbfeChqH1+t1\nmzZtcs45d+GFF7qlS5fWW69bt27uH//4h3POuaefftr98pe/DH6AjdCc+5+IiIg0nu93e6Py1NP/\nDrNZ0/w0oQ4dOvg/V1dXk5WVRUpKCj/4wQ9ITk7GzCguLq5326ioKDye7/5bwsPDOXjwYKPqfvHF\nF1RVVdWJIyEhIWC8wWIsLi7m0KFDdOrU6ZjtCgoK6Ny5c+ATcRy14wPIzs6mW7dueL1evF4vZWVl\n/vNUUFBQbwxQMzxm3rx5AMybN48RI0accEwiIiIiRzv9E2bnmubnBAQa4lC7fP78+SxdupSVK1ey\nf/9+tm/fXvuu+ilx3nnn0apVK3bt2uUvKygoCFg/WIzR0dGcc845bNu27ZjtEhIS2Lp1a71ttmnT\nhq+++sq/XFRUdEyd2udp9erV/P73v2fRokWUlpZSWlpKRESE/zwlJCTUGwPA8OHDWbJkCZs2bSI/\nP58bbrgh4LGKiIiINNbpnzCHUGxsLJ999lmdsqMT4fLyclq3bo3X66WiooIHHngg6FjipuDxeEhP\nT2fKlClUVlaSn5/vfxivPsFiNDMyMzMZP348hYWFVFdXs3btWo4cOcKwYcNYsWIFixYtoqqqipKS\nEjZu3AhA9+7dWbx4MZWVlWzdupWcnJygMZeXlxMWFkZUVBSHDx9m6tSplJeX+9ffeuutTJw40Z+g\nb968mdLSUgDi4+P52c9+xogRI7jpppto3br1SZ0/ERERkdqUMJ+ErKwsHnnkESIjI5k+fTpw7F3n\nkSNHkpiYSHx8PKmpqfTs2bNR+2hMcl277lNPPcX+/fuJi4tj1KhRZGRkBEwkjxdjdnY2P/7xj+nR\nowdRUVFkZWVRXV1NQkICr732GtnZ2URGRnLJJZewadMmAMaNG0dYWBixsbFkZmYyfPjwoMd1zTXX\ncM0119ClSxeSk5MJDw+vM4xk/PjxDBkyhP79+9O+fXtuvfVWKisr/etHjRrFBx98wMiRIxt8vkRE\nREQaQhOXtBBZWVns3buX2bNnhzqUU+Kdd95hxIgRbN++vUnbVf8TERE5s2jiEvHbsmULmzdvBmDd\nunXk5OSQnp4e4qhOjSNHjjBjxgx+/etfhzoUEREROQMpYT5DlZeXk56eTtu2bRk6dCj3338/AwcO\nDHVYTS4/Px+v18vevXsZO3ZsqMMRERGRM5CGZIgEof4nIiJyZtGQDBERERGRJqaEWUREREQkCCXM\nIiIiIiJBKGEWEREREQlCCbOIiIiISBBKmENszJgxPProo01eV0RERESahl4rdxKSk5PJycmhb9++\noQ5FTpHm3P9ERESk8fRauWamqqoq1CGIiIiIyElSwnyCRo4cyc6dOxk4cCARERFkZ2ezY8cOPB4P\nubm5JCUl0a9fPwCGDBlCXFwcXq+X3r1789FHH/nbyczMZNKkSQCsWrWKhIQEpk+fTkxMDPHx8cyZ\nM+eE6paUlDBw4EDat2/PZZddxsSJE+nVq1fA4wkWY58+fcjNzfUv5+Xl1Wnrww8/pH///kRFRREX\nF8fjjz9+YidVREREpBlSwnyC5s6dS2JiIq+++iplZWXcd999/nVvv/02+fn5LF++HIABAwawbds2\n9u3bR1paGsOGDQvYblFREeXl5ezZs4dZs2Zx5513cuDAgUbXveOOO2jXrh379u1jzpw55OXlYRb4\nrw+NiRHwt3Xw4EF+8YtfMGDAAAoLC9m6dav/i4KIiIjImUAJ80k6enyrmfHwww9z7rnn0rp1awBG\njx5NeHg4YWFhTJo0iY0bN1JeXl5ve2effTYTJ07krLPO4rrrrqNt27Zs2bKlUXWrq6tZvHgxU6dO\npXXr1nTt2pVRo0YFPY7GxFjbq6++SlxcHPfccw9nn302bdq0oUePHsfdTkREROR00SrUAZwse+ut\nJmnH9e7dJO0AdOjQwf+5urqaCRMmsGjRIoqLizEzzIzi4mLatWt3zLZRUVF4PN99jwkPD+fgwYP1\n7idQ3S+++IKqqqo6cSQkJASMt7Ex1lZQUEDnzp2D1hERERE5nZ32CXNTJrqNFWiIQ+3y+fPns3Tp\nUlauXEliYiIHDhzA6/We0jcvnHfeebRq1Ypdu3aRkpIC1CS2gRwvxjZt2vDVV1/56xcVFfk/JyQk\nsGDBglN0JCIiIiKhpyEZJyE2NpbPPvusTtnRiXB5eTmtW7fG6/VSUVHBAw88EHQscVPweDykp6cz\nZcoUKisryc/PZ+7cuQHrHy/G7t27s3jxYiorK9m6dSs5OTn+dddffz1FRUU8+eSTHD58mIMHD7Ju\n3bpTenwiIiIi3yclzCchKyuLRx55hMjISKZPnw4ce9d55MiRJCYmEh8fT2pqKj179mzUPhqTXNeu\n+9RTT7F//37i4uIYNWoUGRkZ/jHVRztejOPGjSMsLIzY2FgyMzMZPny4f13btm158803eeWVV4iN\njaVLly681UTDZERERESaA01c0kJkZWWxd+9eZs+eHepQTivqfyIiImcWTVwiflu2bGHz5s0ArFu3\njpycHNLT00MclYiIiMjp57R/6E/qV15eztChQyksLCQmJob777+fgQMHhjosERERkdOOhmSIBKH+\nJyIicmbRkAwRERERkSamhFlEREREJAglzCIiIiIiQShhFhEREREJotm/JSMpKemUz4wnEkhSUlKo\nQxAREZEQa9BbMszsWuBP1NyRznHOPVFPnd7AH4Ew4AvnXJ+GbuurV+9bMkREREREmsopeUuGmXmA\np4FrgB8BQ83soqPqtAf+F7jeOZcKDG7otiKBaIptqY/6hdRH/ULqo34hTaUhY5gvBT51zu1wzh0B\nFgCDjqqTAbzknNsN4JwrbsS2IvXShU7qo34h9VG/kPqoX0hTaUjCHA8U1Fre5SurrQsQaWb/NLP1\nZjaiEduKiIiIiDRbTfXQXysgDegLtAHWmNmaJmpbRERERCRkjvvQn5ldDkxxzl3rW84CXO2H98zs\nv4FznHMP+5ZnAcuA3cfbtlYbeuJPRERERE65xj7015A7zOuBFDNLAgqBW4ChR9VZAjxlZmcBrYHL\ngOnAlgZse0KBi4iIiIh8H46bMDvnqszsLuANvns13MdmdnvNaveMcy7fzJYDm4Aq4Bnn3EcA9W17\nqg5GRERERKSpNeg9zCIiIiIiLVXIp8Y2s2vNLN/MPvGNhRbBzLab2UYze9/M1oU6HgkNM8sxs71m\ntqlWmdfM3jCzLWa23PceeGlBAvSLyWa2y8ze8/1cG8oY5ftnZh3MbKWZfWhmm83sbl+5rhktWD39\n4je+8kZdM0J6h9k3scknQD9gDzXjpW9xzuWHLChpFszsM+CnzrnSUMcioWNmVwEHgbnOuYt9ZU8A\nXzrnpvm+ZHudc1mhjFO+XwH6xWSg3Dk3PaTBSciYWSwQ65zbYGZtgf9QM/dDJrpmtFhB+sV/0Yhr\nRqjvMGtiEwnECH3/lBBzzq0Gjv7SNAjI833OA274XoOSkAvQL6DmuiEtlHOuyDm3wff5IPAx0AFd\nM1q0AP3i2zlBGnzNCHVCoolNJBAHvOmbCOfXoQ5GmpXznXN7oeZCCJwf4nik+bjLzDaY2Sz92b1l\nM7OOQHdgLRCja4ZAnX7xrq+owdeMUCfMIoFc6ZxLAwYAd/r+BCtSHz25LAAzgU7Oue5AETWvNpUW\nyPdn90XAWN8dxaOvEbpmtED19ItGXTNCnTDvBhJrLXfwlUkL55wr9P37BfAyNcN3RAD2mlkM+Mem\n7QtxPNIMOOe+cN89lPMs0COU8UhomFkrapKi/3POLfEV65rRwtXXLxp7zQh1wuyfFMXMzqZmYpNX\nQhyThJiZhfu+CWJmbYD+wAehjUpCyKg7zuwVYLTv8yhqJk6SlqdOv/AlQt9KR9eMlioX+Mg5N6NW\nma4Zcky/aOw1I+TvYfa9xmMG301s8nhIA5KQM7Nkau4qO2om13lO/aJlMrP5QG8gCtgLTAb+BiwE\nEoAdwBDn3P5QxSjfvwD9og81YxOrge3A7d+OW5WWwcyuBN4GNlPz+8MBE4B1wIvomtEiBekXGTTi\nmhHyhFlEREREpDkL9ZAMEREREZFmTQmziIiIiEgQSphFRERERIJQwiwiIiIiEoQSZhERERGRIJQw\ni4iIiIgEoYRZROQMY2Y/N7OloY5DRORMoYRZROTMpJfsi4g0ESXMIiIhYmbDzOxdM3vPzP5sZh4z\nKzez6Wb2gZm9aWZRvrrdzWyNmW0ws5fMrL2vvLOv3gYz+7dvpkyAdma20Mw+NrP/q7XPx31tbzCz\naSE4bBGR044SZhGREDCzi4D/Ano659KomZ51GBAOrHPOpVIznetk3yZ5wP3Oue7AB7XKnwOe8pX3\nBAp95d2Bu4FuQGcz62lmkcANzrlUX/3/OdXHKSJyJlDCLCISGv2ANGC9mb0P9AWSqUmcX/TVmQdc\nZWYRQHvn3GpfeR5wtZm1BeKdc68AOOcOO+e+9tVZ55wrdM45YAPQETgAVJrZLDO7Eag85UcpInIG\nUMIsIhIaBuQ559Kcc5c457o656bWU8/Vqt8Yh2p9rgJaOeeqgEuBRcD1wOuNDVpEpCVSwiwiEhor\ngJvN7DwAM/OaWSJwFnCzr84wYLVzrgwoMbMrfeUjgFXOuYNAgZkN8rVxtpmdG2iHZhYO/MA59zow\nHrj4VByYiMiZplWoAxARaYmccx+b2UPAG2bmAQ4DdwEVwKVmNhHYS804Z4BRwF99CfFnQKavfATw\njJlN9bUxuL7d+f6NAJaY2Tm+5XFNfFgiImckqxneJiIizYGZlTvn2oU6DhER+Y6GZIiINC+6iyEi\n0szoDrOIiIiISBC6wywiIiIiEoQSZhERERGRIJQwi4iIiIgEoYRZRERERCQIJcwiIiIiIkEoYRYR\nERERCeL/ATg1GKbozz1NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f14d8e4cc90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(val_acc_curve,label='validation accuracy')\n",
    "plt.plot(val_auc_curve,label='validation auc')\n",
    "plt.plot(train_acc_curve,label='training accuracy')\n",
    "plt.plot(train_auc_curve,label='training auc')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0.6,0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.844313305113\n"
     ]
    }
   ],
   "source": [
    "# And a full pass over the validation data:\n",
    "val_err = 0\n",
    "Ypred_batches = []\n",
    "Ytrue_batches = []\n",
    "val_batches = 0\n",
    "\n",
    "for batch in iterate_minibatches(X_val, y_val, batch_size, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, y_pred = val_fun(inputs, targets)\n",
    "    Ypred_batches.append(y_pred)\n",
    "    Ytrue_batches.append(targets)\n",
    "\n",
    "    val_err += err\n",
    "    val_batches += 1\n",
    "\n",
    "Ypred_val = np.concatenate(Ypred_batches)\n",
    "Ytrue_val = np.concatenate(Ytrue_batches)\n",
    "val_acc = accuracy_score(Ytrue_val, Ypred_val>0.5)\n",
    "val_auc = roc_auc_score(Ytrue_val, Ypred_val)\n",
    "\n",
    "print val_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[l_0.beta,\n",
       " l_0.gamma,\n",
       " l_0.mean,\n",
       " l_0.inv_std,\n",
       " dense0.W,\n",
       " dense0_bn.beta,\n",
       " dense0_bn.gamma,\n",
       " dense0_bn.mean,\n",
       " dense0_bn.inv_std,\n",
       " dense1.W,\n",
       " dense1_bn.beta,\n",
       " dense1_bn.gamma,\n",
       " dense1_bn.mean,\n",
       " dense1_bn.inv_std,\n",
       " dense2.W,\n",
       " dense2_bn.beta,\n",
       " dense2_bn.gamma,\n",
       " dense2_bn.mean,\n",
       " dense2_bn.inv_std,\n",
       " dense3.W,\n",
       " dense3_bn.beta,\n",
       " dense3_bn.gamma,\n",
       " dense3_bn.mean,\n",
       " dense3_bn.inv_std,\n",
       " dense4.W,\n",
       " dense4_bn.beta,\n",
       " dense4_bn.gamma,\n",
       " dense4_bn.mean,\n",
       " dense4_bn.inv_std,\n",
       " dense_out.W,\n",
       " dense_out.b]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasagne.layers.get_all_params(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from six.moves import cPickle as pickle\n",
    "import lasagne\n",
    "\n",
    "def save(nn, filename):\n",
    "    params = lasagne.layers.get_all_param_values(nn)\n",
    "    with open(filename, 'wb') as fout:\n",
    "        pickle.dump(params, fout, protocol=2)\n",
    "        \n",
    "def save_and_download(nn, filename):\n",
    "    from IPython.display import FileLink\n",
    "    \n",
    "    save(nn, filename)\n",
    "    return FileLink(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='nn9.weights' target='_blank'>nn9.weights</a><br>"
      ],
      "text/plain": [
       "/homeappl/home/trng47/nn9.weights"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_and_download(nn, \"nn9.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "plt.title('Resulting ROC curves')\n",
    "\n",
    "fpr,tpr,_ = roc_curve(Ytrue_val,Ypred_val)\n",
    "plt.plot(fpr,tpr,label='validation ROC, auc=%.5f'%(val_auc))\n",
    "fpr,tpr,_ = roc_curve(Ytrue_train,Ypred_train)\n",
    "plt.plot(fpr,tpr,label='training ROC, auc=%.5f'%(train_auc))\n",
    "\n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "det_predicted = lasagne.layers.get_output(nn, deterministic=True)\n",
    "eval_fun = theano.function([input_X], det_predicted[:,1], allow_input_downcast=True, name=\"eval_NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prob = eval_fun(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1:\n",
    "Add batch normalization after each hidden layer. See if new network does better that the old one.\n",
    "\n",
    "To add Batch Normalization, one can use batch_norm:\n",
    "\n",
    "```\n",
    "normalized_layer = lasagne.layers.batch_norm(previous_layer)\n",
    "```\n",
    "\n",
    "For simplicity, one can implement batch-normalized NN in a new notebook by \n",
    "clicking __File__ -> __Make a copy__ in the jupyter notebook top bar. Do not forget to rename the copy.\n",
    "\n",
    "\n",
    "Alternatively, one can just copy-paste code or start editing\n",
    "\n",
    "\n",
    "__Bonus task 1.1__:\n",
    " - See if batch-normalizing input does you any good.\n",
    " - Which operation is this ~equivalent to in terms of Scikit-Learn?\n",
    " \n",
    " \n",
    "__ After you made it through batch_normalization, scroll to the notebook bottom for the second assignment__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "#### Assuming, you made it through the batch-normalization assignment.\n",
    "\n",
    "__Task2__\n",
    "\n",
    "If you have seen the batch-normalized NN learning curves, you probably have noticed that the validation curves first ascended to a peak and than started to descend while the training score still increase. That's some __overfitting__ happening right in front of you!\n",
    "\n",
    "Your second task, and also a reasonable step towards improving your kaggle ensemble, is to get rid of that overfitting by the tools you know already:\n",
    "\n",
    "- __Early stopping__: stop training when validation score starts decreasing\n",
    "- __Dropout__: force nn to learn redundant representations using [lasagne.layers.DropoutLayer](http://lasagne.readthedocs.io/en/latest/modules/layers/noise.html) (syntax like batch_norm)\n",
    "- __L2 or L1 regularization__: plain old weight penalty still works here. Can be implemented using [lasagne.regularization](http://lasagne.readthedocs.io/en/latest/modules/regularization.html)\n",
    "- __Distortion__: add random (e.g. gaussian) noize to the input data to virtually increase dataset size. \n",
    "   * Can be done via [gaussian noise layer](http://lasagne.readthedocs.io/en/latest/modules/layers/noise.html#lasagne.layers.GaussianNoiseLayer) or manually\n",
    "   \n",
    "   \n",
    "A friendly advice:\n",
    " - You are not restricted to a single approach, however we recommend slow and methodic changes against mixing everything in random proportions right away.\n",
    " - If a method is more mathematically sound or cool than others, it doesn't mean it's bound to score better in competition.\n",
    "   \n",
    "Again, the recommended approach is to copy a notebook and solve the task in the copied version, however you may pick ane approach at your own doom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
